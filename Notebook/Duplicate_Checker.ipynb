{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF computation\n",
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO compute TF-IDF, we need to know the following:\n",
    "**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    ">$TF(t)$ = (Number of times term $t$ appears in a $document$) / (Total number of terms in the $document$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    ">$IDF(t)$ = $log_e$(Total number of $documents$ / Number of $documents$ with term $t$ in it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf(word, blob) computes \"term frequency\" which is the number of times \n",
    "# a word appears in a document blob,normalized by dividing by \n",
    "# the total number of words in blob. \n",
    "# We use TextBlob for breaking up the text into words and getting the word counts.\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "\n",
    "# n_containing(word, bloblist) returns the number of documents containing word.\n",
    "# A generator expression is passed to the sum() function.\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob.words)\n",
    "\n",
    "# idf(word, bloblist) computes \"inverse document frequency\" which measures how common \n",
    "# a word is among all documents in bloblist. \n",
    "# The more common a word is, the lower its idf. \n",
    "# We take the ratio of the total number of documents \n",
    "# to the number of documents containing word, then take the log of that. \n",
    "# Add 1 to the divisor to prevent division by zero.\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "\n",
    "# tfidf(word, blob, bloblist) computes the TF-IDF score. \n",
    "# It is simply the product of tf and idf.\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = pd.read_csv('../input/Questions_Filtered.csv', encoding='latin1')\n",
    "answers = pd.read_csv('../input/Answers_Filtered.csv', encoding='latin1')\n",
    "tags = pd.read_csv('../input/Tags_Filtered.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list = []\n",
    "id_list=[]\n",
    "for index, row in questions.iterrows():\n",
    "    # we append the title to the text body here\n",
    "    question_list.append(TextBlob(str(row[6])+\" \"+str(row[5])))\n",
    "    id_list.append([row[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict={}\n",
    "qID_dict={}\n",
    "for index, question_text in enumerate(question_list):\n",
    "    # For each word in this specific question_text, we will send the word and the question_text\n",
    "    # to find out the tf (Term frequeency will check the frequency of the word in )\n",
    "    if index < 10:\n",
    "        print('Words of interest in Question ID {}'.format(id_list[index]))\n",
    "    scores = {word: tfidf(word, question_text, question_list) for word in question_text.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse = true)\n",
    "    if index < 10:\n",
    "            print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "     # word dict    \n",
    "    if word in tfidf_dict:\n",
    "        tfidf_dict[word].append([idlist[i],round(score, 5)])\n",
    "    else:\n",
    "        tfidf_dict[word] = [[idlist[i],round(score, 5)]]\n",
    "\n",
    "    # qID dict\n",
    "    if idlist[i] in qID_dict:\n",
    "        qID_dict[idlist[i]].append(word)\n",
    "    else:\n",
    "        lst=[]\n",
    "        lst.append(word)\n",
    "        qID_dict[idlist[i]]=lst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
